{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29349088-a39f-4132-a807-04441442ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f7d00e6-e417-4ea6-b472-0e9e1aab5023",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TestData:\n",
    "    value: float\n",
    "    metric_type: str\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Metric {0}: value -> {{{1}}}\".format(self.metric_type, self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0372799b-7f30-49a8-9674-2318f2aaab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(sources):\n",
    "    data = []\n",
    "    for source in sources:\n",
    "        if source.type == 'test':\n",
    "            data.extend([acc for acc in source.results if acc.metric_type == 'accuracy'])\n",
    "        elif source.type == 'project':\n",
    "            data.extend([spd for spd in source.outputs if spd.metric_type == 'completion_time'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba743db3-0a7a-4a8e-9be0-b168f8de722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(raw_data: [TestData]):\n",
    "    normalized = []\n",
    "    max_time = 10000  # Default max time for normalization\n",
    "    for item in raw_data:\n",
    "        if item.metric_type == 'accuracy':\n",
    "            normalized.append({'accuracy': item.value / 100})\n",
    "        elif item.metric_type == 'completion_time':\n",
    "            normalized.append({'completion_time': 1 - (item.value / max_time)})\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eacf0e4-c16d-474f-9b5d-fac2461b4021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_skill_level(normalized_data: list[object]):    \n",
    "    metrics = {'accuracy': [], 'speed': []}\n",
    "    \n",
    "    for item in normalized_data:        \n",
    "        if item.get('accuracy') is not None:\n",
    "            metrics['accuracy'].append(item.get('accuracy'))\n",
    "        elif item.get('completion_time') is not None:\n",
    "            metrics['speed'].append(item.get('completion_time'))\n",
    "    \n",
    "    df_accuracy = pd.Series(metrics['accuracy'])\n",
    "    df_speed = pd.Series(metrics['speed'])\n",
    "\n",
    "    avg_accuracy = 0\n",
    "    avg_speed = 0\n",
    "    \n",
    "    if len(df_accuracy) > 0:\n",
    "        avg_accuracy = df_accuracy.sum() / len(df_accuracy)\n",
    "        print(\"Added to average accuracy, NEW: {0} => avg_accuracy == {1}\".format(df_accuracy.sum(), avg_accuracy))\n",
    "    if len(df_speed) > 0:\n",
    "        avg_speed = df_speed.sum() / len(df_speed)\n",
    "        print(\"Added to average speed, NEW: {0} => avg_speed == {1}\".format(df_speed.sum(), avg_speed))\n",
    "    \n",
    "    return (avg_accuracy * 0.7) + (avg_speed * 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01051ae8-ff31-493c-8304-654dc36845c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_level(score):\n",
    "    if score < 0.5:\n",
    "        return \"Novice\"\n",
    "    elif score < 0.8:\n",
    "        return \"Intermediate\"\n",
    "    else:\n",
    "        return \"Advanced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ef7452-a5c6-4a99-8399-1299c1a7c02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data Collected: [TestData(value=75, metric_type='accuracy'), TestData(value=90, metric_type='accuracy'), TestData(value=82, metric_type='accuracy'), TestData(value=3600, metric_type='completion_time'), TestData(value=7200, metric_type='completion_time')]\n",
      "Normalized Data: [{'accuracy': 0.75}, {'accuracy': 0.9}, {'accuracy': 0.82}, {'completion_time': 0.64}, {'completion_time': 0.28}]\n",
      "Added to average accuracy, NEW: 2.4699999999999998 => avg_accuracy == 0.8233333333333333\n",
      "Added to average speed, NEW: 0.92 => avg_speed == 0.46\n",
      "Skill Score: 0.71433333\n",
      "Assigned Level: Intermediate\n"
     ]
    }
   ],
   "source": [
    "# Sample usage - performance analysis pipeline\n",
    "\n",
    "# Create some test data\n",
    "test1 = TestData(value=75, metric_type='accuracy')\n",
    "test2 = TestData(value=82, metric_type='accuracy')\n",
    "test3 = TestData(value=90, metric_type='accuracy')\n",
    "\n",
    "project1 = TestData(value=3600, metric_type='completion_time')  # 1 hour\n",
    "project2 = TestData(value=7200, metric_type='completion_time')  # 2 hours\n",
    "\n",
    "sources = [\n",
    "    type('test_source', (), {'type': 'test', 'results': {test1, test2, test3}})(),\n",
    "    type('project_source', (), {'type': 'project', 'outputs': {project1, project2}})()\n",
    "]\n",
    "\n",
    "# Collect and process data\n",
    "raw_data = collect_data(sources)\n",
    "print(f\"Raw Data Collected: {raw_data}\")\n",
    "\n",
    "normalized = normalize_data(raw_data)\n",
    "print(f\"Normalized Data: {normalized}\")\n",
    "\n",
    "skill_score = calculate_skill_level(normalized)\n",
    "print(f\"Skill Score: {skill_score:.8f}\")\n",
    "\n",
    "level = assign_level(skill_score)\n",
    "print(f\"Assigned Level: {level}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682941dd-1080-4b6d-afbe-48124980ae13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
